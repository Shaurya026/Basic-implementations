{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <center><h1>\n",
    " Experiment #6 \n",
    "</h1>\n",
    "<b>AIM:</b> Build an artificial neural network by implementing the Backpropagation algorithm and test the same appropriate data set -->\n",
    "\n",
    "# Building a Neural Network from scratch and working on Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "* IMPORTING THE NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be applying a neural network on breast cancer data set where value of :\n",
    "* 0 => Malignant\n",
    "* 1 => Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "d = load_breast_cancer()\n",
    "print(d['data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is our row-wise data for breast cancer prediciton, we need to normalize our data for our NN to achieve the minima quickly at an high learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 2.057e+01, 1.969e+01, ..., 1.660e+01, 2.060e+01,\n",
       "        7.760e+00],\n",
       "       [1.038e+01, 1.777e+01, 2.125e+01, ..., 2.808e+01, 2.933e+01,\n",
       "        2.454e+01],\n",
       "       [1.228e+02, 1.329e+02, 1.300e+02, ..., 1.083e+02, 1.401e+02,\n",
       "        4.792e+01],\n",
       "       ...,\n",
       "       [2.654e-01, 1.860e-01, 2.430e-01, ..., 1.418e-01, 2.650e-01,\n",
       "        0.000e+00],\n",
       "       [4.601e-01, 2.750e-01, 3.613e-01, ..., 2.218e-01, 4.087e-01,\n",
       "        2.871e-01],\n",
       "       [1.189e-01, 8.902e-02, 8.758e-02, ..., 7.820e-02, 1.240e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['data'].transpose()\n",
    "# each row represents a column's data so we  can normalize a row here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 569)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the shape of our data \n",
    "d['data'].transpose().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85143641 0.97354347 0.93189455 ... 0.78565005 0.97496332 0.36726774]\n",
      " [0.35103145 0.60094691 0.71863375 ... 0.94961109 0.99188367 0.82989516]\n",
      " [0.84859374 0.9183885  0.89834842 ... 0.74839334 0.96814318 0.33114505]\n",
      " ...\n",
      " [0.91202749 0.63917526 0.83505155 ... 0.48728522 0.91065292 0.        ]\n",
      " [0.90695841 0.54208555 0.71220185 ... 0.43721664 0.80563769 0.56593732]\n",
      " [0.77987669 0.58389086 0.57444576 ... 0.51292142 0.81332809 0.46169487]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1\n",
      "  0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1\n",
      "  0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1\n",
      "  1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1\n",
      "  0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
      "  1 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0\n",
      "  1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      "  1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
      "  1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      "  0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1\n",
      "  0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0\n",
      "  1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# NORMALIZING THE DATASET b/w 0 and 1: \n",
    "x = d['data'].transpose().copy()\n",
    "for i in range(len(x)):\n",
    "    x[i] = (x[i])/(np.max(x[i]) - np.min(x[i])) \n",
    "    \n",
    "print(x)\n",
    "\n",
    "# looking at the output values \n",
    "y = d['target'].copy()\n",
    "y = y.reshape((1,len(y)))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the dataset for Hold-Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the data set uniformly in 70-30 ratio \n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(x.T, y.T, train_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 398)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Making "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining activation functions: \n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def ReLU(z):\n",
    "    #zero = np.zeros(z.shape)\n",
    "    #return np.max(zero,z) # returns element wise maximum\n",
    "    z[z < 0] = 0\n",
    "    return z\n",
    "def ReLU_derivative(z):\n",
    "    z[z > 0] = 1\n",
    "    z[z <= 0] = 0\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_i(prev_activation,W ,b):\n",
    "    z = np.dot(W,prev_activation) + b\n",
    "    g = ReLU(z)\n",
    "    data_node = {'w':W, 'b':b, 'z':z, 'a':g}\n",
    "    return data_node\n",
    "\n",
    "def LOSS(Y_true,Y_pred):\n",
    "    return (-1/Y_true.shape[1]) * np.sum(Y_true*np.log(Y_pred) + (1-Y_true)*np.log(1-Y_pred))\n",
    "\n",
    "def Backward_Prop(w_plus, dz_plus, z_curr, input_val, length):\n",
    "    da = np.dot(w_plus.T,dz_plus)\n",
    "    dz = np.multiply(da,ReLU_derivative(z_curr))\n",
    "    dw = np.dot(dz, input_val.T)\n",
    "    db = (1/length) * np.sum(dz,axis = 1, keepdims = True)\n",
    "    return {'dz':dz,'dw':dw,'db':db}\n",
    "    \n",
    "def LOGISTIC_REGRESSION_NEURAL_NET(input_values, output_values, learn_rate, num_iterations):\n",
    "    length = input_values.shape[1]\n",
    "    np.random.seed(2)\n",
    "    l1 = {'w':np.random.rand(8,input_values.shape[0]) * 0.01, 'b': np.zeros((8,1))}\n",
    "    l2 = {'w':np.random.rand(4,8) * 0.01, 'b': np.zeros((4,1))}\n",
    "    l3 = {'w':np.random.rand(2,4) * 0.01, 'b': np.zeros((2,1))}\n",
    "    l4 = {'w':np.random.rand(1,2) * 0.01, 'b': np.zeros((1,1))}\n",
    "    \n",
    "    cost_values = []\n",
    "    \n",
    "    print('layer1:','weights dimensions',l1['w'].shape)\n",
    "    print('layer2:','weights dimensions',l2['w'].shape,l2['b'].shape)\n",
    "    print('layer3:','weights dimensions',l3['w'].shape,l3['b'].shape)\n",
    "    print('layer4:','weights dimensions',l4['w'].shape,l4['b'].shape)\n",
    "    #print(cost)\n",
    "    \n",
    "    for i in range(1,num_iterations+1):\n",
    "        l1 = layer_i(input_values,l1['w'],l1['b'])\n",
    "        l2 = layer_i(l1['a'],l2['w'],l2['b'])\n",
    "        l3 = layer_i(l2['a'],l3['w'],l3['b'])\n",
    "        l4 = layer_i(l3['a'],l4['w'],l4['b'])\n",
    "        # l4 is the output so we need to change it's activation values\n",
    "        l4['a'] = sigmoid(l4['z'])\n",
    "        cost = LOSS(output_values,l4['a'])\n",
    "        cost_values.append(cost)\n",
    "        \n",
    "        # defining backprop for the last layer manually since the activation function is different \n",
    "        dz = l4['a'] - output_values\n",
    "        dw = np.dot(dz,l3['a'].T)\n",
    "        db = (1/length) * np.sum(dz,axis = 1,keepdims = True)\n",
    "        \n",
    "        # Now backpropagating in other layers:\n",
    "        dl3 = Backward_Prop(l4['w'],dz,l3['z'],l2['a'],length)\n",
    "        dl2 = Backward_Prop(l3['w'],dl3['dz'],l2['z'],l1['a'],length)\n",
    "        dl1 = Backward_Prop(l2['w'],dl2['dz'],l1['z'],input_values,length)\n",
    "        \n",
    "        # updating the values:\n",
    "        l4['w'] = l4['w'] - learn_rate*dw\n",
    "        l4['b'] = l4['b'] - learn_rate*db\n",
    "        l3['w'] = l3['w'] - learn_rate*dl3['dw']\n",
    "        l3['b'] = l3['b'] - learn_rate*dl3['db']\n",
    "        l2['w'] = l2['w'] - learn_rate*dl2['dw']\n",
    "        l2['b'] = l2['b'] - learn_rate*dl2['db']\n",
    "        l1['w'] = l1['w'] - learn_rate*dl1['dw']\n",
    "        l1['b'] = l1['b'] - learn_rate*dl1['db']\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('COST AFTER',i,'ITERATIONS ====>',cost)\n",
    "    return {'l1':l1,'l2':l2,'l3':l3,'l4':l4}, cost_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the MODEL for learning on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1: weights dimensions (8, 30)\n",
      "layer2: weights dimensions (4, 8) (4, 1)\n",
      "layer3: weights dimensions (2, 4) (2, 1)\n",
      "layer4: weights dimensions (1, 2) (1, 1)\n",
      "COST AFTER 100 ITERATIONS ====> 0.6807235124690812\n",
      "COST AFTER 200 ITERATIONS ====> 0.6729272323963584\n",
      "COST AFTER 300 ITERATIONS ====> 0.6663040507288249\n",
      "COST AFTER 400 ITERATIONS ====> 0.33673326887368554\n",
      "COST AFTER 500 ITERATIONS ====> 0.30814776740201616\n",
      "COST AFTER 600 ITERATIONS ====> 0.2870794896605691\n"
     ]
    }
   ],
   "source": [
    "layers, cost = LOGISTIC_REGRESSION_NEURAL_NET(X_train.T,Y_train.T,0.001,600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> looking at the learning of model through loss :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13002378970>]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = range(len(cost))\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(a,cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function for performance measures:\n",
    "def confusion_mat(predicted,Y_test):\n",
    "    ans = {'TP':0, 'TN':0, 'FP':0, 'FN':0}\n",
    "    for i,j in zip(Y_test,predicted):\n",
    "        if i == 0 and j == 0:\n",
    "            ans['TN'] += 1\n",
    "        elif i ==1 and j == 1:\n",
    "            ans['TP'] += 1\n",
    "        elif i == 0 and j == 1:\n",
    "            ans['FP'] += 1\n",
    "        else:\n",
    "            ans['FN'] += 1\n",
    "    return ans\n",
    "\n",
    "def Recall(ans):\n",
    "    return (ans['TP'])/(ans['TP'] + ans['FN'])\n",
    "def Precision(ans):\n",
    "    return (ans['TP'])/(ans['TP'] + ans['FP'])\n",
    "def accuracy(ans):\n",
    "    return (ans['TP'] + ans['TN'])/(ans['TP'] + ans['TN'] + ans['FN'] + ans['FP'])\n",
    "def F_measure(ans):\n",
    "    return (2*Precision(ans)*Recall(ans))/(Precision(ans) + Recall(ans))\n",
    "def misclassification_rate(ans):\n",
    "    return (ans['FP'] + ans['FN'])/(ans['TP'] + ans['TN'] + ans['FN'] + ans['FP'])\n",
    "def FPR(ans):\n",
    "    return ans['FP']/(ans['FP'] + ans['TN'])\n",
    "def G_measure(ans):\n",
    "    return (2*Recall(ans)*(1-FPR(ans)))/(Recall(ans) + (1-FPR(ans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics on Training set : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct predicitons: 393\n",
      "Total False predicitons: 5\n",
      "accuracy : 0.9874371859296482\n"
     ]
    }
   ],
   "source": [
    "a = (Y_train.T == np.round(layers['l4']['a']))\n",
    "l = a.shape[1]\n",
    "T = 0\n",
    "F = 0\n",
    "for i in a[0]:\n",
    "    if i:\n",
    "        T += 1\n",
    "    else:\n",
    "        F += 1\n",
    "print(\"Total correct predicitons:\",T)\n",
    "print(\"Total False predicitons:\",F)\n",
    "print('accuracy :', T/l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: {'TP': 247, 'TN': 146, 'FP': 3, 'FN': 2}\n",
      "Recall: 0.9919678714859438\n",
      "Precision: 0.988\n",
      "accuracy: 0.9874371859296482\n",
      "F-measure: 0.9899799599198397\n",
      "G-measure: 0.9858796834205886\n"
     ]
    }
   ],
   "source": [
    "train = confusion_mat(np.round(layers['l4']['a'][0]), Y_train.T[0])\n",
    "print('confusion matrix:',train)\n",
    "print('Recall:', Recall(train))\n",
    "print('Precision:', Precision(train))\n",
    "print('accuracy:', accuracy(train))\n",
    "print('F-measure:', F_measure(train))\n",
    "print('G-measure:', G_measure(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics on Test Set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      "  0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      "  1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      "  1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      "  1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      "  1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      "  0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# making predicitons on test set :\n",
    "prev_activation = X_test.T\n",
    "g = None\n",
    "for i,v in enumerate(layers.values()):\n",
    "    z = np.dot(v['w'],prev_activation) + v['b']\n",
    "    g = ReLU(z)\n",
    "    prev_activation = g\n",
    "    if i == 3:\n",
    "        g = sigmoid(z)\n",
    "print(np.round(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: {'TP': 105, 'TN': 60, 'FP': 3, 'FN': 3}\n",
      "Recall: 0.9722222222222222\n",
      "Precision: 0.9722222222222222\n",
      "accuracy: 0.9649122807017544\n",
      "F-measure: 0.9722222222222222\n",
      "G-measure: 0.9621993127147767\n"
     ]
    }
   ],
   "source": [
    "test = confusion_mat(np.round(g[0]), Y_test.T[0])\n",
    "print('confusion matrix:',test)\n",
    "print('Recall:', Recall(test))\n",
    "print('Precision:', Precision(test))\n",
    "print('accuracy:', accuracy(test))\n",
    "print('F-measure:', F_measure(test))\n",
    "print('G-measure:', G_measure(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct predicitons: 165\n",
      "Total False predicitons: 6\n",
      "accuracy : 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "a = (Y_test.T == np.round(g))\n",
    "l = a.shape[1]\n",
    "T = 0\n",
    "F = 0\n",
    "for i in a[0]:\n",
    "    if i:\n",
    "        T += 1\n",
    "    else:\n",
    "        F += 1\n",
    "print(\"Total correct predicitons:\",T)\n",
    "print(\"Total False predicitons:\",F)\n",
    "print('accuracy :', T/l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at model in keras : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying on tensorflow : \n",
    "import tensorflow as tf\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-2,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.9)\n",
    "# opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "Model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units = 8, activation = tf.nn.relu, input_shape = [30,]),\n",
    "    tf.keras.layers.Dense(units = 4, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units = 2, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units = 1, activation = tf.nn.sigmoid)\n",
    "])\n",
    "Model.compile(optimizer = opt, loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 0s 888us/step - loss: 0.6774 - acc: 0.6098\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6450 - acc: 0.6274\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6108 - acc: 0.6274\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5791 - acc: 0.6274\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 888us/step - loss: 0.5533 - acc: 0.6292\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5273 - acc: 0.7188\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4952 - acc: 0.8278\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4608 - acc: 0.8647\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4259 - acc: 0.8524\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3917 - acc: 0.8981\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3569 - acc: 0.8998\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3263 - acc: 0.9104\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2850 - acc: 0.9192\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.2503 - acc: 0.9350\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.2226 - acc: 0.9297\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.2077 - acc: 0.9438\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1963 - acc: 0.9402\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1851 - acc: 0.9385\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1772 - acc: 0.9420\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1697 - acc: 0.9420\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1633 - acc: 0.9543\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.1571 - acc: 0.9490\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.1520 - acc: 0.9508\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1484 - acc: 0.9561\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1464 - acc: 0.9455\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1394 - acc: 0.9561\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.1342 - acc: 0.9525\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1342 - acc: 0.9561\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1280 - acc: 0.9596\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1256 - acc: 0.9525\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1237 - acc: 0.9561\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1187 - acc: 0.9649\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1161 - acc: 0.9631\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1153 - acc: 0.9596\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1123 - acc: 0.9596\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1103 - acc: 0.9596\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1119 - acc: 0.9525\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1086 - acc: 0.9596\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1054 - acc: 0.9649\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1017 - acc: 0.9666\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1003 - acc: 0.9649\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0990 - acc: 0.9631\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0968 - acc: 0.9701\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0977 - acc: 0.9684\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0953 - acc: 0.9631\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0962 - acc: 0.9684\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0927 - acc: 0.9666\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0897 - acc: 0.9754\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0888 - acc: 0.9666\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0878 - acc: 0.9754\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0875 - acc: 0.9666\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0924 - acc: 0.9666\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0871 - acc: 0.9701\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0837 - acc: 0.9719\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0842 - acc: 0.9719\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0833 - acc: 0.9719\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0814 - acc: 0.9701\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0797 - acc: 0.9772\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0781 - acc: 0.9789\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0817 - acc: 0.9684\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0805 - acc: 0.9684\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0774 - acc: 0.9754\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0757 - acc: 0.9772\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0754 - acc: 0.9772\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0779 - acc: 0.9754\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0757 - acc: 0.9772\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0750 - acc: 0.9736\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0738 - acc: 0.9772\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0756 - acc: 0.9701\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0720 - acc: 0.9772\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0722 - acc: 0.9789\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0719 - acc: 0.9736\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0725 - acc: 0.9754\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0726 - acc: 0.9772\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0700 - acc: 0.9754\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0694 - acc: 0.9807\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0702 - acc: 0.9859\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0691 - acc: 0.9772\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0726 - acc: 0.9754\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0684 - acc: 0.9824\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0670 - acc: 0.9789\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0670 - acc: 0.9807\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0675 - acc: 0.9719\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0678 - acc: 0.9824\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0658 - acc: 0.9789\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0655 - acc: 0.9807\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0659 - acc: 0.9789\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0656 - acc: 0.9824\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0652 - acc: 0.9807\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0643 - acc: 0.9807\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0646 - acc: 0.9789\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0646 - acc: 0.9772\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0657 - acc: 0.9789\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0636 - acc: 0.9772\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0641 - acc: 0.9736\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0644 - acc: 0.9807\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0619 - acc: 0.9807\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0639 - acc: 0.9807\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0639 - acc: 0.9772\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0623 - acc: 0.9772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13014a25310>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.fit(x.T, y.T, epochs=100,verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see Adam did much better than SGD i.e. achieved minima much quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
